{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3cfHQyCCHjed"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pymorphy3\n",
        "!pip install catboost"
      ],
      "metadata": {
        "id": "QV_mKPC_IzoP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from catboost import Pool, CatBoostClassifier\n",
        "\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
        "import torch.nn.functional as F\n",
        "import json\n",
        "import torch\n",
        "\n",
        "import nltk\n",
        "import string\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "import pymorphy3\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "nltk.download('stopwords')\n",
        "russian_stopwords = stopwords.words(\"russian\")\n",
        "np.random.seed(0)\n",
        "torch.manual_seed(0)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False"
      ],
      "metadata": {
        "id": "wffm7DDsIxKH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import clear_output\n",
        "\n",
        "\n",
        "def plot_progress(train_losses, val_loss, train_accs, val_accs, lrs):\n",
        "    clear_output(True)\n",
        "\n",
        "    f, (ax1, ax2, ax3) = plt.subplots(nrows=1, ncols=3)\n",
        "    f.set_figheight(6)\n",
        "    f.set_figwidth(16)\n",
        "\n",
        "    ax1.plot(train_losses, label='train loss')\n",
        "    ax1.plot(val_loss, label='test loss')\n",
        "    ax1.plot(np.zeros_like(train_losses), '--', label='zero')\n",
        "    ax1.set_title('Loss', fontsize=14)\n",
        "    ax1.set_ylabel('Loss')\n",
        "    ax1.set_xlabel('Batch number')\n",
        "    ax1.legend()\n",
        "\n",
        "    ax2.plot(train_accs, label='Train Auc')\n",
        "    ax2.plot(val_accs, label='Val Auc')\n",
        "    ax2.plot(np.ones_like(train_accs), '--', label='Accuracy')\n",
        "    ax2.set_title('Auc', fontsize=14)\n",
        "    ax2.set_ylabel('Auc')\n",
        "    ax2.set_xlabel('Batch number')\n",
        "    ax2.legend()\n",
        "\n",
        "    ax3.plot(lrs, label='learning rate')\n",
        "    ax3.set_title('Learing rate')\n",
        "    ax3.set_xlabel('Batch number')\n",
        "    ax3.legend()\n",
        "\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "9J0ERsIDJhv5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "patterns = \"\".join(string.punctuation)\n",
        "stopwords_ru = stopwords.words(\"russian\")\n",
        "morph = pymorphy3.MorphAnalyzer()\n",
        "\n",
        "\n",
        "def lemmatize(doc):\n",
        "    for i in string.punctuation:\n",
        "        doc = doc.replace(i, ' ')\n",
        "    tokens = []\n",
        "    for token in doc.split():\n",
        "        if token and token not in stopwords_ru:\n",
        "            token = token.strip()\n",
        "            #token = morph.normal_forms(token)[0]\n",
        "            tokens.append(token)\n",
        "    return ' '.join(tokens)"
      ],
      "metadata": {
        "id": "uyUfQWECJakg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "geo = pd.read_csv('/content/drive/MyDrive/data/geo_info.csv', delimiter=';')\n",
        "train = pd.read_csv('/content/drive/MyDrive/data/train.csv', delimiter=';')\n",
        "labels = pd.read_csv('/content/drive/MyDrive/data/train_labels.csv', delimiter=';')\n",
        "vectors = pd.read_csv('/content/drive/MyDrive/data/referer_vectors.csv', delimiter=';')"
      ],
      "metadata": {
        "id": "9gzrdUms4tVO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test = pd.read_csv('/content/drive/MyDrive/data/test.csv', delimiter=';')\n",
        "test_users = pd.read_csv('/content/drive/MyDrive/data/test_users.csv', delimiter=';')"
      ],
      "metadata": {
        "id": "Q0RLtZ9KBmbn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train.info()"
      ],
      "metadata": {
        "id": "2TEGYmwY6u1y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "geo.info() #region_id - много пропусков"
      ],
      "metadata": {
        "id": "i5fq_F1x62Bt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectors.info()"
      ],
      "metadata": {
        "id": "UEDbitTy67HG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Merge all dataframes"
      ],
      "metadata": {
        "id": "KkbVn8wD7kDD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train.drop_duplicates(inplace=True)\n",
        "vectors.drop_duplicates(inplace=True)"
      ],
      "metadata": {
        "id": "ZTYYyGRP9daJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.merge(train, geo, how='left', on=\"geo_id\")\n",
        "df = pd.merge(df, vectors, how='inner', on='referer')\n",
        "df = pd.merge(df, labels, how='left', on='user_id')\n",
        "df = df[~df.target.isna()]\n",
        "df.head()"
      ],
      "metadata": {
        "id": "U9wiqPj77Ezj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "\n",
        "domains = defaultdict(int)\n",
        "for i in df.referer:\n",
        "    domains[i.split('/')[2]] += 1\n",
        "sorted_domains = {k: v for k, v in sorted(domains.items(), key=lambda item: item[1])}\n",
        "cnt = sum(1 for i in sorted_domains.values() if i < 2)\n",
        "cnt"
      ],
      "metadata": {
        "collapsed": true,
        "id": "8J-61uG-FXC2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(domains)"
      ],
      "metadata": {
        "id": "Tq6ez8A5GHwl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sorted_domains = {k: v for k, v in sorted(domains.items(), key=lambda item: item[1])}\n",
        "cnt = sum(1 for i in sorted_domains.values() if i < 2)\n",
        "cnt"
      ],
      "metadata": {
        "id": "-5Ej-UGzG2WD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Baseline\n",
        "\n",
        "roc-auc = 0.8972650049187818 - all user_agent + cats from geo + domain"
      ],
      "metadata": {
        "id": "mtxZ5WlC-KT0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prepare df for train\n",
        "\n",
        "add user_agent info + domain"
      ],
      "metadata": {
        "id": "hVl0--IwoSik"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_new = df.copy()\n",
        "df_new.index = df_new.user_id\n",
        "df_new['domain'] = [i.split('/')[2] for i in df_new.referer]\n",
        "second_part_domain = []\n",
        "for i in df_new.referer:\n",
        "    a = i.split('/')\n",
        "    if a[3] != '':\n",
        "        second_part_domain.append(a[3])\n",
        "    else:\n",
        "        second_part_domain.append('')\n",
        "df_new['second_domain'] = second_part_domain\n",
        "df_new.drop(columns=['request_ts', 'user_id', 'geo_id', 'referer'], inplace=True)"
      ],
      "metadata": {
        "id": "4do5S1WMj5zz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "browsers = []\n",
        "versions = []\n",
        "os_version = []\n",
        "os = []\n",
        "for i in tqdm(df_new.user_agent):\n",
        "    try:\n",
        "        a = json.loads(str(i).replace(\"'\", '\"'))\n",
        "        browsers.append(a['browser'])\n",
        "        os.append(a['os'])\n",
        "        versions.append(a['browser_version'])\n",
        "        os_version.append(a['os_version'])\n",
        "    except:\n",
        "        a, b, c, d = browsers[-1],  os[-1], versions[-1], os_version[-1]\n",
        "        browsers.append(a)\n",
        "        os.append(b)\n",
        "        versions.append(c)\n",
        "        os_version.append(d)\n",
        "df_new['browser'] = browsers\n",
        "df_new['os'] = os\n",
        "df_new['browser_version'] = versions\n",
        "df_new['os_version'] = os_version\n",
        "df_new.drop(columns=['user_agent'], inplace=True)"
      ],
      "metadata": {
        "id": "2FwmaKgHIYZq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_new['os'].value_counts()"
      ],
      "metadata": {
        "id": "UQzLk3cGL1D1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# os_to_common = {'Fedora': 'Linux', 'Ubuntu': 'Linux', 'Tizen': 'Phone', 'Chrome OS': 'Linux', 'Windows Phone': 'Phone', 'FreeBSD': 'Other', 'Chromecast': 'Other'}\n",
        "# df_new['os'] = df_new['os'].apply(lambda x: os_to_common[x] if x in os_to_common else x)"
      ],
      "metadata": {
        "id": "GfVoEVmwLwA7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "timezone_modes = {}\n",
        "for k in tqdm(list(df_new.timezone_id.unique())):\n",
        "    res = df_new[df_new.timezone_id == k].region_id.mode()\n",
        "    try:\n",
        "        timezone_modes[k] = res[0]\n",
        "    except KeyError:\n",
        "        timezone_modes[k] = df_new.region_id.mode()[0]"
      ],
      "metadata": {
        "collapsed": true,
        "id": "_0NWarLa0XHz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "regions = []\n",
        "for i in tqdm(df_new.values):\n",
        "    if pd.isna(i[1]):\n",
        "        regions.append(timezone_modes[i[2]])\n",
        "    else:\n",
        "        regions.append(i[1])\n",
        "df_new['region_id'] = regions"
      ],
      "metadata": {
        "id": "fnyTywBsw_fV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test2 = test.copy()\n",
        "test2 = pd.merge(test2, geo, how='left', on=\"geo_id\")\n",
        "test2 = pd.merge(test2, vectors, how='inner', on='referer')\n",
        "test2['domain'] = [i.split('/')[2] for i in test2.referer]\n",
        "\n",
        "test2 = pd.merge(test_users, test2, how='inner', on='user_id')\n",
        "test2 = test2[~test2.user_id.duplicated()]\n",
        "test2.index = test2.user_id\n",
        "\n",
        "regions = []\n",
        "for i in tqdm(test2.values):\n",
        "    if pd.isna(i[6]):\n",
        "        if i[7] not in timezone_modes:\n",
        "            regions.append(test2.region_id.mode()[0])\n",
        "        else:\n",
        "            regions.append(timezone_modes[i[7]])\n",
        "    else:\n",
        "        regions.append(i[6])\n",
        "test2['region_id'] = regions\n",
        "\n",
        "second_part_domain = []\n",
        "for i in test2.referer:\n",
        "    a = i.split('/')\n",
        "    if a[3] != '':\n",
        "        second_part_domain.append(a[3])\n",
        "    else:\n",
        "        second_part_domain.append('')\n",
        "test2['second_domain'] = second_part_domain\n",
        "test2.drop(columns=['request_ts', 'user_id', 'geo_id', 'referer'], inplace=True)\n",
        "\n",
        "browsers = []\n",
        "versions = []\n",
        "os_version = []\n",
        "os = []\n",
        "for i in tqdm(test2.user_agent):\n",
        "    try:\n",
        "        a = json.loads(str(i).replace(\"'\", '\"'))\n",
        "        browsers.append(a['browser'])\n",
        "        os.append(a['os'])\n",
        "        versions.append(a['browser_version'])\n",
        "        os_version.append(a['os_version'])\n",
        "    except:\n",
        "        a, b, c, d = browsers[-1],  os[-1], versions[-1], os_version[-1]\n",
        "        browsers.append(a)\n",
        "        os.append(b)\n",
        "        versions.append(c)\n",
        "        os_version.append(d)\n",
        "test2['browser'] = browsers\n",
        "test2['os'] = os\n",
        "test2['browser_version'] = versions\n",
        "test2['os_version'] = os_version\n",
        "\n",
        "test2.drop(columns=['user_agent'], inplace=True)\n",
        "\n",
        "test2.head()"
      ],
      "metadata": {
        "id": "MB0QJexY7E-p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in ['browser', 'os_version', 'browser_version', 'domain', 'second_domain', 'country_id', 'timezone_id', 'region_id']:\n",
        "    all_elems = list(set(test2[i]) - set(df_new[i]))\n",
        "    print(len(all_elems), i)"
      ],
      "metadata": {
        "id": "evLzyhhD5-5P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#le = LabelEncoder()\n",
        "#for i in ['domain']:\n",
        "#    all_elems = list(set(test2[i]) | set(df_new[i]))\n",
        "#    le.fit(all_elems)\n",
        "#    df_new[i] = le.transform(df_new[i])\n",
        "#    test2[i] = le.transform(test2[i])"
      ],
      "metadata": {
        "id": "ZzGxiZH_o-1a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#замена label encoder для domain через евклидово расстояние по датасету vectors\n",
        "def euclidean_distance(vec1, vec2):\n",
        "    return np.linalg.norm(np.array(vec1) - np.array(vec2))\n",
        "\n",
        "\n",
        "vectors['domain'] = [i.split('/')[2] for i in vectors.referer]\n",
        "domains = {}\n",
        "for elem in tqdm(df_new.domain.unique()):\n",
        "    domains[elem] = vectors.loc[vectors.domain == elem, [f'component{i}' for i in range(10)]].mean().astype(int).values.tolist()\n",
        "\n",
        "domains_test = {}\n",
        "for elem in tqdm(test2.domain.unique()):\n",
        "    domains_test[elem] = vectors.loc[vectors.domain == elem, [f'component{i}' for i in range(10)]].mean().astype(int).values.tolist()\n",
        "res_test_domains = {}\n",
        "for key in tqdm(domains_test.keys()):\n",
        "    closest_object = None\n",
        "    min_distance = float('inf')\n",
        "    for name, vector in domains.items():\n",
        "        distance = euclidean_distance(domains_test[key], vector)\n",
        "        if distance < min_distance:\n",
        "            min_distance = distance\n",
        "            closest_object = name\n",
        "    res_test_domains[key] = closest_object\n",
        "\n",
        "for k, v in res_test_domains.items():\n",
        "    test2.loc[test2.domain == k, 'domain'] = v\n",
        "set(test2.domain) - set(df_new.domain)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "7adN3vDwp-uY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test2.loc[test2.browser == 'HbbTV', 'browser'] = 'Chrome'\n",
        "set(test2.browser) - set(df_new.browser)"
      ],
      "metadata": {
        "id": "X0om3c-rnX0S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(set(test2.country_id) - set(df_new.country_id))\n",
        "len(test2[test2.country_id == '10cdeb5']) + len(test2[test2.country_id == 'db21ba']) # всего 3 семлпа - заполним модой\n",
        "test2.loc[(test2.country_id == '10cdeb5') | (test2.country_id == 'db21ba'), 'country_id'] = test2.country_id.mode()[0]\n",
        "set(test2.country_id) - set(df_new.country_id)"
      ],
      "metadata": {
        "id": "UjNn8XJHnwZD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(set(test2.timezone_id) - set(df_new.timezone_id))\n",
        "len(test2[test2.timezone_id == '10480cf']) + len(test2[test2.timezone_id == '98e66e']) # 3 семлпа\n",
        "test2.loc[(test2.timezone_id == '10480cf') | (test2.timezone_id == '98e66e'), 'timezone_id'] = test2.timezone_id.mode()[0]\n",
        "set(test2.timezone_id) - set(df_new.timezone_id)"
      ],
      "metadata": {
        "id": "sX68nsuEoTgQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(set(test2.region_id) - set(df_new.region_id))\n",
        "len(test2[test2.region_id == '354493']) + len(test2[test2.region_id == '19bf7a']) + len(test2[test2.region_id == '575134'])# 3 семлпа\n",
        "test2.loc[(test2.region_id == '354493') | (test2.region_id == '19bf7a') | (test2.region_id == '575134'), 'region_id'] = test2.region_id.mode()[0]\n",
        "set(test2.region_id) - set(df_new.region_id)"
      ],
      "metadata": {
        "id": "_NQEVal3oh1c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# различия => можно оставить browser, country, time, region\n",
        "'''1 browser\n",
        "6 os_version\n",
        "75 browser_version\n",
        "228 domain\n",
        "13903 second_domain\n",
        "2 country_id\n",
        "2 timezone_id\n",
        "3 region_id'''"
      ],
      "metadata": {
        "id": "Jvs0urCznGlH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "columns = ['country_id', 'region_id', 'domain', 'browser', 'os', 'timezone_id'] + [f'component{i}' for i in range(10)]\n",
        "category = ['region_id', 'timezone_id', 'country_id', 'os', 'browser', 'domain']\n",
        "X, y = df_new[columns], df_new['target']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)"
      ],
      "metadata": {
        "id": "BrX0D2IMeef4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train['os'].value_counts()"
      ],
      "metadata": {
        "id": "kCMSoweKTeRK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Catboost Model"
      ],
      "metadata": {
        "id": "OvC1SXBW257W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = Pool(data=X_train,\n",
        "                     label=y_train,\n",
        "                     cat_features=category)\n",
        "\n",
        "eval_dataset = Pool(data=X_test,\n",
        "                    label=y_test,\n",
        "                    cat_features=category)\n",
        "\n",
        "\n",
        "model = CatBoostClassifier(\n",
        "    iterations=700,\n",
        "    learning_rate=0.01,\n",
        "    random_seed=42,\n",
        "    max_depth=6,\n",
        "    # loss_function = 'CrossEntropy',\n",
        "    custom_metric=['AUC']\n",
        ")\n",
        "model.fit(\n",
        "    train_dataset,\n",
        "    eval_set=eval_dataset,\n",
        "    use_best_model=True,\n",
        "    early_stopping_rounds=30,\n",
        "    verbose=True\n",
        ")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "QuaZ4c3TK_kd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preds_proba = model.predict_proba(eval_dataset)[:, 1]\n",
        "roc_auc_score(y_test, preds_proba)"
      ],
      "metadata": {
        "id": "ReLQSEFAP4N9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "importances = model.get_feature_importance(type='PredictionValuesChange')\n",
        "feature_importances = pd.Series(importances, index=X.columns).sort_values()\n",
        "feature_importances"
      ],
      "metadata": {
        "id": "WJmlpTEFf44l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "component0      0.800107\n",
        "timezone_id     0.849820\n",
        "component9      1.025836\n",
        "component4      1.099582\n",
        "component7      1.105323\n",
        "component6      1.968028\n",
        "region_id       2.149833\n",
        "component5      2.486600\n",
        "country_id      2.971283\n",
        "component3      3.159298\n",
        "component1      3.643665\n",
        "component2      4.152498\n",
        "component8      5.700547\n",
        "os              7.060173\n",
        "browser        16.055573\n",
        "domain         45.771834"
      ],
      "metadata": {
        "id": "JdQ9ZSfWLl5v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 0.88"
      ],
      "metadata": {
        "id": "duApk7RTLizT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_X = X_train.copy()\n",
        "train_X.columns"
      ],
      "metadata": {
        "id": "5jud4wO38zFp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_X['country_id'] = train_X['country_id'].astype('category')\n",
        "train_X['region_id'] = train_X['region_id'].astype('category')\n",
        "train_X['timezone_id'] = train_X['timezone_id'].astype('category')\n",
        "train_X['domain'] = train_X['domain'].astype('category')\n",
        "# train_X['second_domain'] = train_X['second_domain'].astype('category')\n",
        "train_X['browser'] = train_X['browser'].astype('category')\n",
        "train_X['os'] = train_X['os'].astype('category')\n",
        "# train_X['browser_vesion'] = train_X['browser_version'].astype('category')\n",
        "# train_X['os_version'] = train_X['os_version'].astype('category')\n",
        "train_X.info()"
      ],
      "metadata": {
        "id": "TuSOG6o_8epy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train.info()"
      ],
      "metadata": {
        "id": "1tbh1Dwe9vFQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from xgboost import XGBClassifier\n",
        "\n",
        "# Define the model\n",
        "model = XGBClassifier(\n",
        "    n_estimators=700,          # Equivalent to iterations\n",
        "    learning_rate=0.01,        # Same as CatBoost's learning_rate\n",
        "    random_state=42,           # Same as random_seed\n",
        "    max_depth=6,               # Same as CatBoost's max_depth\n",
        "    eval_metric='auc',         # Equivalent to custom_metric=['AUC']\n",
        "    use_label_encoder=False,     # To avoid warnings about label encoding\n",
        "    enable_categorical = True\n",
        ")\n",
        "\n",
        "# Fit the model\n",
        "model.fit(\n",
        "    train_X,\n",
        "    y_train,\n",
        "    # eval_set=eval_dataset,\n",
        "    verbose=True\n",
        ")"
      ],
      "metadata": {
        "id": "6KzZYxDb7oqc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_X = X_test.copy()\n",
        "test_X['country_id'] = test_X['country_id'].astype('category')\n",
        "test_X['region_id'] = test_X['region_id'].astype('category')\n",
        "test_X['timezone_id'] = test_X['timezone_id'].astype('category')\n",
        "test_X['domain'] = test_X['domain'].astype('category')\n",
        "# train_X['second_domain'] = train_X['second_domain'].astype('category')\n",
        "test_X['browser'] = test_X['browser'].astype('category')\n",
        "test_X['os'] = test_X['os'].astype('category')\n",
        "# train_X['browser_vesion'] = train_X['browser_version'].astype('category')\n",
        "# train_X['os_version'] = train_X['os_version'].astype('category')\n",
        "test_X.info()"
      ],
      "metadata": {
        "id": "CjOQlWls-zg1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preds_proba = model.predict_proba(test_X)[:, 1]\n",
        "roc_auc_score(y_test, preds_proba)"
      ],
      "metadata": {
        "id": "ungwHUmL-gnh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Define the model\n",
        "model = XGBClassifier(\n",
        "    n_estimators=700,\n",
        "    learning_rate=0.01,\n",
        "    random_state=42,\n",
        "    max_depth=6,\n",
        "    eval_metric='auc',\n",
        "    use_label_encoder=False,\n",
        "    enable_categorical=True\n",
        ")\n",
        "\n",
        "# Define the parameter grid for fine-tuning\n",
        "param_grid = {\n",
        "    'n_estimators': [700, 900],\n",
        "    'learning_rate': [0.01, 0.05],\n",
        "    'max_depth': [4, 6, 8],\n",
        "    'min_child_weight': [1, 3, 5],  # Regularization parameter\n",
        "    'subsample': [0.6,0.8, 1.0],    # Proportion of samples to use for training\n",
        "    'colsample_bytree': [0.6, 0.8, 1.0]  # Proportion of features to use for training\n",
        "}\n",
        "\n",
        "# Set up GridSearchCV\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=model,\n",
        "    param_grid=param_grid,\n",
        "    scoring='roc_auc',         # Use AUC as the scoring metric\n",
        "    cv=3,                      # Number of cross-validation folds\n",
        "    verbose=1,\n",
        "    n_jobs=-1                 # Use all available cores\n",
        ")\n",
        "\n",
        "# Fit the model with GridSearchCV\n",
        "grid_search.fit(train_X, y_train)\n",
        "\n",
        "# Get the best parameters and best score\n",
        "best_params = grid_search.best_params_\n",
        "best_score = grid_search.best_score_\n",
        "\n",
        "print(\"Best Parameters:\", best_params)\n",
        "print(\"Best AUC Score:\", best_score)\n"
      ],
      "metadata": {
        "id": "uys-1C5j_h3a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stacking"
      ],
      "metadata": {
        "id": "VLSMPyKR3HAL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "columns = ['country_id', 'region_id', 'domain', 'browser', 'os', 'timezone_id'] + [f'component{i}' for i in range(10)]\n",
        "category = ['region_id', 'timezone_id', 'country_id', 'os', 'browser', 'domain']\n",
        "X, y = df_new[columns], df_new['target']\n",
        "for cat in category:\n",
        "    X[cat] = X[cat].astype('category')\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "vXAHH-Ik3Jb7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import xgboost as xgb\n",
        "import lightgbm as lgbm\n",
        "\n",
        "\n",
        "xgb_model = xgb.XGBClassifier(n_estimators = 300, enable_categorical=True, random_seed=42, verbosity=1, learning_rate = 0.25, max_depth = 6, min_chile_weight=2)\n",
        "lgbm_model = lgbm.LGBMClassifier(n_estimators = 300, random_seed=42, categorical_feature=category, verbose=1, learning_rate = 0.25, num_leaves = 63)\n",
        "cat_model = CatBoostClassifier(cat_features=category, iterations = 700, depth = 6, l2_leaf_reg = 5, verbose=True, random_seed=42)"
      ],
      "metadata": {
        "id": "fG1IZdpo3MMz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# список базовых моделей\n",
        "from sklearn.ensemble import ExtraTreesClassifier, RandomForestClassifier, StackingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.svm import LinearSVC\n",
        "import xgboost as xgb\n",
        "\n",
        "\n",
        "estimators = [\n",
        "\n",
        "    (\"XGBoost\", xgb_model),\n",
        "    (\"LightGBM\", lgbm_model),\n",
        "    (\"CatBoost\", cat_model),\n",
        "\n",
        "    # То, что не дало прироста в ансамбле\n",
        "    # (\"SVM\", make_pipeline(preprocessor, LinearSVC(verbose=False))),\n",
        "    # (\"MLP\", make_pipeline(preprocessor, MLPClassifier(verbose=False, hidden_layer_sizes=(100, 30, ), alpha=0.001,random_state=75, max_iter = 1300, ))),\n",
        "\n",
        "]\n",
        "\n",
        "# в качестве мета-модели будем использовать LogisticRegression\n",
        "# meta_model = StackingClassifier(\n",
        "#     estimators=estimators,\n",
        "#     final_estimator=LogisticRegression(random_state=42, verbose=True),\n",
        "#     # final_estimator=RandomForestClassifier(n_estimators = 10_000,\n",
        "#                                            # max_depth = 5,\n",
        "#                                            # verbose=False),\n",
        "#     n_jobs=-1,\n",
        "#     verbose=True,\n",
        "# )\n",
        "\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "meta_model = StackingClassifier(\n",
        "    estimators=estimators,\n",
        "    final_estimator=xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42),\n",
        "    n_jobs=-1,\n",
        "    verbose=True,\n",
        ")\n",
        "\n",
        "stacking_classifier = meta_model\n",
        "stacking_classifier"
      ],
      "metadata": {
        "id": "nsU4qdpB3I4A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stacking_classifier.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "O_6KQNSi8V1C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# corr_df = pd.DataFrame()\n",
        "\n",
        "# for model, (name, _) in zip(stacking_classifier.estimators_, stacking_classifier.estimators):\n",
        "#     preprocessed = stacking_classifier.estimators[0][1].steps[0][1].fit(X_train, y_train).transform(X_test)\n",
        "#     print(name, 'roc-auc: ', round(roc_auc_score(y_test, model.predict_proba(X_test)[:, 1]), 4))\n",
        "\n",
        "#     corr_df[name] = model.predict(X_test)\n"
      ],
      "metadata": {
        "id": "-M78G08f3M-0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('ensemble score:', round(roc_auc_score(y_test, stacking_classifier.predict_proba(X_test)[:, 1]), 4))"
      ],
      "metadata": {
        "id": "V3C6ew543Q2A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Make test prediction"
      ],
      "metadata": {
        "id": "HjRMR4UAlxDP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_users"
      ],
      "metadata": {
        "id": "TdR3Ko-0ziXa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test2.head()"
      ],
      "metadata": {
        "id": "vthXSzOycHqI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test2.info()"
      ],
      "metadata": {
        "id": "XOfI6Y344tjv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test2['country_id'] = test2['country_id'].astype('category')\n",
        "test2['region_id'] = test2['region_id'].astype('category')\n",
        "test2['timezone_id'] = test2['timezone_id'].astype('category')\n",
        "test2['domain'] = test2['domain'].astype('category')\n",
        "test2['second_domain'] = test2['second_domain'].astype('category')\n",
        "test2['browser'] = test2['browser'].astype('category')\n",
        "test2['os'] = test2['os'].astype('category')\n",
        "test2['browser_vesion'] = test2['browser_version'].astype('category')\n",
        "test2['os_version'] = test2['os_version'].astype('category')\n",
        "test2.info()"
      ],
      "metadata": {
        "id": "VLHo9LLU42qF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_preds = stacking_classifier.predict_proba(test2[columns])[:, 1]"
      ],
      "metadata": {
        "id": "5xfgs-ET4GWM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_preds.shape"
      ],
      "metadata": {
        "id": "k8fABRmbAX3C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_users.shape"
      ],
      "metadata": {
        "id": "XaVNzWHHjb6j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_users['target'] = test_preds"
      ],
      "metadata": {
        "id": "5sMtqVaTAat6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_users"
      ],
      "metadata": {
        "id": "r2CGwssgAcwY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_users.to_csv('baseline_fix_cat_features.csv', index=False, sep=';')"
      ],
      "metadata": {
        "id": "xlueKjPvFWIi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nLTBi5KW5oEu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}